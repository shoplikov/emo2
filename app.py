from __future__ import annotations
import io
import tempfile
from dataclasses import dataclass
from pathlib import Path
from typing import Generator, List, Optional, Tuple

import cv2
import numpy as np
import pandas as pd
import streamlit as st
import torch
from PIL import Image
from transformers import AutoImageProcessor, AutoModelForImageClassification
from ultralytics import YOLO

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Page config
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.set_page_config(page_title="Video Emotion Analysis & Review", page_icon="üé≠", layout="wide")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Constants & state
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
EMOTION_OPTIONS: List[str] = ["Angry", "Disgust", "Fear", "Happy", "Sad", "Surprise", "Neutral"]

STATE = {
    "page": "page",
    "csv": "processed_csv",
    "video": "video_path",
    "review_df": "review_data",
    "idx": "current_transition",
    "show_corr": "show_correction",
}

PAGES = {"processing": "processing", "review": "review"}

# Defaults
for key, default in [
    (STATE["page"], PAGES["processing"]),
    (STATE["csv"], None),
    (STATE["video"], None),
    (STATE["review_df"], None),
    (STATE["idx"], 0),
]:
    if key not in st.session_state:
        st.session_state[key] = default


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Types
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@dataclass
class ModelBundle:
    yolo: YOLO
    processor: AutoImageProcessor
    expr_model: AutoModelForImageClassification
    device: torch.device
    amp_dtype: Optional[torch.dtype]


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Device and models
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def get_device(use_gpu: bool = True) -> torch.device:
    return torch.device("cuda") if (use_gpu and torch.cuda.is_available()) else torch.device("cpu")


@st.cache_resource(show_spinner=False)
def load_models(use_gpu: bool = True, use_fp16: bool = True) -> Optional[ModelBundle]:
    """
    Load YOLO face detector and ViT expression classifier with GPU/AMP settings.
    Cached per (use_gpu, use_fp16) to let you switch in the sidebar.
    """
    device = get_device(use_gpu)

    # Perf knobs
    if device.type == "cuda":
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.allow_tf32 = True
        try:
            torch.backends.cuda.matmul.allow_tf32 = True  # type: ignore[attr-defined]
        except Exception:
            pass
        try:
            torch.set_float32_matmul_precision("high")  # PyTorch 2.x
        except Exception:
            pass

    try:
        # YOLO (Ultralytics)
        yolo = YOLO("models/yolov12n-face.pt")  # make sure the file exists locally
        try:
            yolo.to("cuda" if device.type == "cuda" else "cpu")
        except Exception:
            pass
        try:
            yolo.fuse()  # small speedup if supported
        except Exception:
            pass

        # HF classifier
        processor = AutoImageProcessor.from_pretrained("trpakov/vit-face-expression")
        expr_model = AutoModelForImageClassification.from_pretrained("trpakov/vit-face-expression").to(device).eval()

        amp_dtype = torch.float16 if (device.type == "cuda" and use_fp16) else None
        return ModelBundle(yolo=yolo, processor=processor, expr_model=expr_model, device=device, amp_dtype=amp_dtype)

    except Exception as e:
        st.error(f"Failed to load models: {e}")
        return None


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Video utilities
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def get_video_info(video_path: str) -> Tuple[float, int, float, int, int]:
    """Return (fps, frame_count, duration_sec, width, height)."""
    cap = cv2.VideoCapture(video_path)
    try:
        fps = float(cap.get(cv2.CAP_PROP_FPS)) or 0.0
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 0)
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 0)
        duration = (frame_count / fps) if fps > 0 else 0.0
        return fps, frame_count, duration, width, height
    finally:
        cap.release()


def iterate_frames(video_path: str, fps: float, duration: float, step_sec: float) -> Generator[Tuple[float, np.ndarray], None, None]:
    """Yield (cur_time_sec, frame_rgb_np) every step_sec."""
    cap = cv2.VideoCapture(video_path)
    try:
        cur_time = 0.0
        while cur_time < duration:
            frame_idx = int(round(cur_time * fps))
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ok, frame_bgr = cap.read()
            if not ok:
                break
            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
            yield cur_time, frame_rgb
            cur_time += step_sec
    finally:
        cap.release()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Batched inference
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _crop_face_from_result(frame_rgb: np.ndarray, box_xyxy: np.ndarray, expand_ratio: float = 0.10) -> Optional[Image.Image]:
    h, w = frame_rgb.shape[:2]
    x1, y1, x2, y2 = box_xyxy
    bw, bh = x2 - x1, y2 - y1
    x1e = max(0, int(x1 - bw * expand_ratio))
    y1e = max(0, int(y1 - bh * expand_ratio))
    x2e = min(w, int(x2 + bw * expand_ratio))
    y2e = min(h, int(y2 + bh * expand_ratio))
    face = frame_rgb[y1e:y2e, x1e:x2e]
    if face.size == 0:
        return None
    return Image.fromarray(face)


def detect_faces_batch(
    yolo_model: YOLO,
    frames_rgb: List[np.ndarray],
    device: torch.device,
    conf: float = 0.25,
    imgsz: int = 960,
    half: bool = True,
    batch: int = 16,
) -> List[Optional[Image.Image]]:
    """frames_rgb -> list of PIL face crops (or None)"""
    if len(frames_rgb) == 0:
        return []
    ultra_device = 0 if device.type == "cuda" else "cpu"
    results = yolo_model.predict(
        source=frames_rgb,
        conf=conf,
        imgsz=imgsz,
        max_det=1,
        device=ultra_device,
        half=(half and device.type == "cuda"),
        verbose=False,
        batch=batch,
    )
    faces: List[Optional[Image.Image]] = []
    for frame, res in zip(frames_rgb, results):
        boxes = getattr(res, "boxes", None)
        if boxes is None or boxes.xyxy is None or boxes.xyxy.shape[0] == 0:
            faces.append(None)
        else:
            box = boxes.xyxy[0].detach().cpu().numpy()
            faces.append(_crop_face_from_result(frame, box))
    return faces


def predict_expressions_batch(
    processor: AutoImageProcessor,
    expr_model: AutoModelForImageClassification,
    faces: List[Optional[Image.Image]],
    device: torch.device,
    amp_dtype: Optional[torch.dtype] = torch.float16,
) -> List[str]:
    """faces -> list of emotion labels; 'no_face' where crop is None."""
    valid_imgs = [img for img in faces if img is not None]
    if len(valid_imgs) == 0:
        return ["no_face"] * len(faces)

    inputs = processor(images=valid_imgs, return_tensors="pt").to(device)

    with torch.no_grad():
        if device.type == "cuda" and amp_dtype is not None:
            with torch.cuda.amp.autocast(dtype=amp_dtype):
                logits = expr_model(**inputs).logits
        else:
            logits = expr_model(**inputs).logits

    preds = logits.softmax(dim=1).argmax(dim=1).tolist()
    labels = [expr_model.config.id2label[i] for i in preds]

    # reinsert Nones
    out: List[str] = []
    j = 0
    for img in faces:
        if img is None:
            out.append("no_face")
        else:
            out.append(labels[j])
            j += 1
    return out


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Core processing
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def process_video(
    video_path: str,
    models: ModelBundle,
    step_sec: float = 1.0,
    batch_size: int = 16,
    imgsz: int = 960,
    conf: float = 0.25,
    progress_cb: Optional[callable] = None,
) -> pd.DataFrame:
    """
    Batched processing to maximize GPU utilization.
    Returns DataFrame: time_from, time_to, emotion_from, emotion_to
    """
    fps, frame_count, duration, *_ = get_video_info(video_path)
    if fps <= 0 or frame_count == 0 or duration == 0:
        raise RuntimeError("–ù–µ–≤–∞–ª–∏–¥–Ω–æ–µ –≤–∏–¥–µ–æ: FPS/frames/duration = 0.")

    # collect per-step emotions
    per_step: List[Tuple[float, str]] = []

    total_steps = max(1, int(np.ceil(duration / step_sec)))
    done_steps = 0

    times_buf: List[float] = []
    frames_buf: List[np.ndarray] = []

    for t, frame_rgb in iterate_frames(video_path, fps, duration, step_sec):
        times_buf.append(t)
        frames_buf.append(frame_rgb)

        if len(frames_buf) >= batch_size:
            _run_batch(times_buf, frames_buf, models, imgsz, conf, batch_size, per_step)
            done_steps += len(frames_buf)
            if progress_cb:
                progress_cb(min(done_steps / total_steps, 1.0))
            times_buf.clear()
            frames_buf.clear()

    if frames_buf:
        _run_batch(times_buf, frames_buf, models, imgsz, conf, batch_size, per_step)
        done_steps += len(frames_buf)
        if progress_cb:
            progress_cb(min(done_steps / total_steps, 1.0))
        times_buf.clear()
        frames_buf.clear()

    # Build transitions
    per_step.sort(key=lambda x: x[0])
    final = []
    prev_emotion: Optional[str] = None
    segment_start = 0.0
    for t, emotion in per_step:
        if prev_emotion is None:
            prev_emotion = emotion
            segment_start = t
        elif emotion != prev_emotion:
            final.append({
                "time_from": round(segment_start, 2),
                "time_to": round(t, 2),
                "emotion_from": prev_emotion,
                "emotion_to": emotion,
            })
            segment_start = t
            prev_emotion = emotion

    if prev_emotion is not None:
        final.append({
            "time_from": round(segment_start, 2),
            "time_to": round(duration, 2),
            "emotion_from": prev_emotion,
            "emotion_to": prev_emotion,
        })

    return pd.DataFrame(final)


def _run_batch(
    times_batch: List[float],
    frames_batch: List[np.ndarray],
    models: ModelBundle,
    imgsz: int,
    conf: float,
    batch_size: int,
    collect: List[Tuple[float, str]],
) -> None:
    faces = detect_faces_batch(
        models.yolo,
        frames_batch,
        models.device,
        conf=conf,
        imgsz=imgsz,
        half=(models.amp_dtype is torch.float16),
        batch=batch_size,
    )
    emotions = predict_expressions_batch(models.processor, models.expr_model, faces, models.device, models.amp_dtype)
    for t, e in zip(times_batch, emotions):
        collect.append((float(t), str(e)))


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Frame & clip helpers
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@st.cache_data(show_spinner=False)
def extract_frame_at_time(video_path: str, time_sec: float) -> Optional[np.ndarray]:
    fps, *_ = get_video_info(video_path)
    if fps <= 0:
        return None
    frame_idx = int(round(time_sec * fps))
    cap = cv2.VideoCapture(video_path)
    try:
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ok, frame_bgr = cap.read()
        if not ok:
            return None
        return cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
    finally:
        cap.release()


def extract_video_clip(video_path: str, start_time: float, end_time: float, output_path: str) -> None:
    fps, _, _, w, h = get_video_info(video_path)
    if fps <= 0:
        raise RuntimeError("Invalid FPS; cannot write clip.")
    cap = cv2.VideoCapture(video_path)
    try:
        start_frame = int(round(start_time * fps))
        end_frame = int(round(end_time * fps))
        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))
        try:
            for _ in range(max(0, end_frame - start_frame)):
                ok, frame = cap.read()
                if not ok:
                    break
                out.write(frame)
        finally:
            out.release()
    finally:
        cap.release()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# UI: Processing page
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def page_processing(perf_opts: dict) -> None:
    st.title("üé≠ –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π –Ω–∞ –≤–∏–¥–µ–æ")
    st.markdown("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤–∏–¥–µ–æ—Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ—Ö–æ–¥–æ–≤")

    uploaded_file = st.file_uploader(
        "–í—ã–±–µ—Ä–∏—Ç–µ –≤–∏–¥–µ–æ—Ñ–∞–π–ª",
        type=["mp4", "avi", "mov", "mkv"],
        help="–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: MP4, AVI, MOV, MKV",
    )

    if uploaded_file is not None:
        # Persist uploaded file to temp path
        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(uploaded_file.name).suffix or ".mp4") as tmp:
            tmp.write(uploaded_file.read())
            st.session_state[STATE["video"]] = tmp.name

        fps, frame_count, duration, *_ = get_video_info(st.session_state[STATE["video"]])
        c1, c2, c3 = st.columns(3)
        c1.metric("–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å", f"{duration:.2f} —Å–µ–∫")
        c2.metric("–ö–∞–¥—Ä–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É", f"{fps:.2f}")
        c3.metric("–í—Å–µ–≥–æ –∫–∞–¥—Ä–æ–≤", frame_count)

        st.subheader("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        # step_sec = st.slider("–ò–Ω—Ç–µ—Ä–≤–∞–ª –∞–Ω–∞–ª–∏–∑–∞ (—Å–µ–∫—É–Ω–¥—ã)", 0.5, 5.0, 1.0, 0.5)
        step_sec = 1.0

        if st.button("üîÑ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤–∏–¥–µ–æ", type="primary"):
            with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π‚Ä¶"):
                models = load_models(use_gpu=perf_opts["use_gpu"], use_fp16=perf_opts["use_fp16"])
            if models is None:
                st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–µ–π.")
                return

            pbar = st.progress(0)
            status = st.empty()
            status.text("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ‚Ä¶")

            try:
                df = process_video(
                    video_path=st.session_state[STATE["video"]],
                    models=models,
                    step_sec=step_sec,
                    batch_size=perf_opts["batch_size"],
                    imgsz=perf_opts["yolo_imgsz"],
                    conf=perf_opts["yolo_conf"],
                    progress_cb=pbar.progress,
                )
                # Save CSV temp for review page
                video_path = st.session_state[STATE["video"]]
                video_name = Path(video_path).stem
                csv_name = f"{video_name}.csv"
                csv_path = str(Path(tempfile.gettempdir()) / csv_name)
                df.to_csv(csv_path, index=False)
                st.session_state[STATE["csv"]] = csv_path

                status.text("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
                st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                c1, c2 = st.columns(2)
                c1.metric("–í—Å–µ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤", len(df))
                unique_emotions = set(df["emotion_from"].tolist() + df["emotion_to"].tolist())
                c2.metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —ç–º–æ—Ü–∏–π", len(unique_emotions))

                st.subheader("üìã –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ —ç–º–æ—Ü–∏–π")
                st.dataframe(df, use_container_width=True)

                buf = io.StringIO()
                df.to_csv(buf, index=False)
                st.download_button(
                    label="üì• –°–∫–∞—á–∞—Ç—å CSV",
                    data=buf.getvalue(),
                    file_name=csv_name,
                    mime="text/csv",
                )

                if st.button("‚û°Ô∏è –ü–µ—Ä–µ–π—Ç–∏ –∫ –ø—Ä–æ–≤–µ—Ä–∫–µ", type="primary"):
                    st.session_state[STATE["page"]] = PAGES["review"]
                    st.rerun()

            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∏–¥–µ–æ: {e}")
                status.text("–û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å!")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# UI: Review page
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def ensure_review_df() -> Optional[pd.DataFrame]:
    csv_path = st.session_state.get(STATE["csv"])
    if not csv_path:
        return None
    if st.session_state.get(STATE["review_df"]) is None:
        base = pd.read_csv(csv_path)
        base = base.copy()
        base["review"] = "pending"
        base["reviewed_emotion_from"] = ""
        base["reviewed_emotion_to"] = ""
        base["comment"] = ""
        st.session_state[STATE["review_df"]] = base
    return st.session_state[STATE["review_df"]]


def page_review() -> None:
    st.title("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ —ç–º–æ—Ü–∏–π")

    if not st.session_state.get(STATE["csv"]):
        st.warning("–ù–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –°–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –≤–∏–¥–µ–æ.")
        if st.button("‚Üê –ù–∞–∑–∞–¥ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ"):
            st.session_state[STATE["page"]] = PAGES["processing"]
            st.rerun()
        return

    df = ensure_review_df()
    if df is None or df.empty:
        st.warning("–ù–µ—Ç –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏.")
        return

    idx: int = int(st.session_state.get(STATE["idx"], 0))
    idx = max(0, min(idx, len(df) - 1))

    st.progress((idx + 1) / len(df))
    st.markdown(f"**–ü–µ—Ä–µ—Ö–æ–¥ {idx + 1} –∏–∑ {len(df)}**")

    row = df.iloc[idx]
    st.subheader(f"–ü–µ—Ä–µ—Ö–æ–¥: {row['emotion_from']} ‚Üí {row['emotion_to']}")
    st.markdown(f"**–í—Ä–µ–º—è:** —Å {row['time_from']}—Å –ø–æ {row['time_to']}—Å")

    c1, c2 = st.columns([2, 2])
    with c1:
        st.markdown("**–ù–∞—á–∞–ª—å–Ω—ã–π –∫–∞–¥—Ä**")
        start = extract_frame_at_time(st.session_state[STATE["video"]], float(row["time_from"]))
        if start is not None:
            st.image(start, caption=f"–í {row['time_from']}—Å", use_container_width=True)
        else:
            st.info("–ù–µ—Ç –∫–∞–¥—Ä–∞ –¥–ª—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.")
    with c2:
        st.markdown("**–ö–æ–Ω–µ—á–Ω—ã–π –∫–∞–¥—Ä**")
        end = extract_frame_at_time(st.session_state[STATE["video"]], float(row["time_to"]))
        if end is not None:
            st.image(end, caption=f"–í {row['time_to']}—Å", use_container_width=True)
        else:
            st.info("–ù–µ—Ç –∫–∞–¥—Ä–∞ –¥–ª—è –∫–æ–Ω–µ—á–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.")

    st.divider()

    st.subheader("–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ—Ö–æ–¥–∞")
    st.markdown("–û–¥–æ–±—Ä–∏—Ç–µ –ø–µ—Ä–µ—Ö–æ–¥, –µ—Å–ª–∏ –æ–Ω –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω, –∏–ª–∏ –æ—Ç–∫–ª–æ–Ω–∏—Ç–µ –∏ –≤–Ω–µ—Å–∏—Ç–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è.")

    if row["review"] != "pending":
        status_color = "üü¢" if row["review"] == "approved" else "üî¥"
        st.markdown(f"**–°—Ç–∞—Ç—É—Å:** {status_color} {row['review'].title()}")
        if row["reviewed_emotion_from"] or row["reviewed_emotion_to"]:
            st.markdown(f"**–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ:** {row['reviewed_emotion_from']} ‚Üí {row['reviewed_emotion_to']}")
        if row["comment"]:
            st.markdown(f"**–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:** {row['comment']}")

    b1, b2 = st.columns(2)
    with b1:
        if st.button("‚úÖ –û–¥–æ–±—Ä–∏—Ç—å", type="primary", use_container_width=True):
            df.loc[idx, "review"] = "approved"
            st.session_state[STATE["review_df"]] = df
            if idx < len(df) - 1:
                st.session_state[STATE["idx"]] = idx + 1
            st.rerun()
    with b2:
        if st.button("‚ùå –û—Ç–∫–ª–æ–Ω–∏—Ç—å", use_container_width=True):
            df.loc[idx, "review"] = "rejected"
            st.session_state[STATE["review_df"]] = df
            st.session_state[STATE["show_corr"]] = True
            st.rerun()

    if row["review"] == "rejected" or st.session_state.get(STATE["show_corr"], False):
        st.subheader("–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è")
        st.markdown("–í—ã–±–µ—Ä–∏—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —ç–º–æ—Ü–∏–∏ –∏ –¥–æ–±–∞–≤—å—Ç–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏.")

        c1, c2 = st.columns(2)
        with c1:
            corrected_from = st.selectbox(
                "–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —ç–º–æ—Ü–∏—è (–æ—Ç):",
                options=[""] + EMOTION_OPTIONS,
                index=0 if not row["reviewed_emotion_from"] else EMOTION_OPTIONS.index(row["reviewed_emotion_from"]) + 1,
            )
        with c2:
            corrected_to = st.selectbox(
                "–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —ç–º–æ—Ü–∏—è (–∫):",
                options=[""] + EMOTION_OPTIONS,
                index=0 if not row["reviewed_emotion_to"] else EMOTION_OPTIONS.index(row["reviewed_emotion_to"]) + 1,
            )

        comment = st.text_area("–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:", value=row["comment"])

        if st.button("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ"):
            df.loc[idx, "reviewed_emotion_from"] = corrected_from
            df.loc[idx, "reviewed_emotion_to"] = corrected_to
            df.loc[idx, "comment"] = comment
            st.session_state[STATE["review_df"]] = df
            st.session_state[STATE["show_corr"]] = False
            st.success("–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ!")
            if idx < len(df) - 1:
                st.session_state[STATE["idx"]] = idx + 1
            st.rerun()

    st.divider()

    st.subheader("–ù–∞–≤–∏–≥–∞—Ü–∏—è")
    n1, n2, n3 = st.columns([1, 2, 1])
    with n1:
        if st.button("‚¨ÖÔ∏è –ü—Ä–µ–¥—ã–¥—É—â–∏–π", disabled=(idx == 0)):
            st.session_state[STATE["idx"]] = max(0, idx - 1)
            st.rerun()
    with n2:
        jump_to = st.number_input("–ü–µ—Ä–µ–π—Ç–∏ –∫ –ø–µ—Ä–µ—Ö–æ–¥—É:", min_value=1, max_value=len(df), value=idx + 1)
        if int(jump_to) - 1 != idx:
            st.session_state[STATE["idx"]] = int(jump_to) - 1
            st.rerun()
    with n3:
        if st.button("‚û°Ô∏è –°–ª–µ–¥—É—é—â–∏–π", disabled=(idx == len(df) - 1)):
            st.session_state[STATE["idx"]] = min(len(df) - 1, idx + 1)
            st.rerun()

    st.divider()

    st.subheader("–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    approved = int((df["review"] == "approved").sum())
    rejected = int((df["review"] == "rejected").sum())
    pending = int((df["review"] == "pending").sum())

    c1, c2, c3 = st.columns(3)
    c1.metric("‚úÖ –û–¥–æ–±—Ä–µ–Ω–æ", approved)
    c2.metric("‚ùå –û—Ç–∫–ª–æ–Ω–µ–Ω–æ", rejected)
    c3.metric("‚è≥ –í –æ–∂–∏–¥–∞–Ω–∏–∏", pending)

    buf = io.StringIO()
    df.to_csv(buf, index=False)
    st.download_button(
        label="üì• –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∏—Ç–æ–≥–æ–≤—ã–π CSV",
        data=buf.getvalue(),
        file_name="reviewed_emotion_transitions.csv",
        mime="text/csv",
        type="primary",
    )

    if st.button("‚Üê –ù–∞–∑–∞–¥ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ"):
        st.session_state[STATE]["page"] = PAGES["processing"]
        st.session_state[STATE]["csv"] = None
        st.session_state[STATE]["review_df"] = None
        st.session_state[STATE]["idx"] = 0
        st.rerun()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Main router + sidebar
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def main() -> None:
    st.sidebar.title("–ù–∞–≤–∏–≥–∞—Ü–∏—è")
    if st.sidebar.button("üìπ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ", use_container_width=True):
        st.session_state[STATE["page"]] = PAGES["processing"]
    if st.sidebar.button("‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤", use_container_width=True):
        st.session_state[STATE["page"]] = PAGES["review"]

    # Perf controls
    st.sidebar.divider()
    st.sidebar.subheader("‚öôÔ∏è –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
    use_gpu = st.sidebar.checkbox("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU (CUDA)", value=torch.cuda.is_available())
    use_fp16 = st.sidebar.checkbox("FP16/AMP", value=True)
    batch_size = st.sidebar.slider("Batch size", 4, 64, 16, step=4)
    # yolo_imgsz = st.sidebar.select_slider("YOLO img size", options=[640, 768, 896, 960, 1024, 1280], value=960)
    # yolo_conf = st.sidebar.slider("YOLO conf", 0.10, 0.60, 0.25, step=0.05)
    yolo_imgsz = 1280
    yolo_conf = 0.25
    perf_opts = {
        "use_gpu": use_gpu,
        "use_fp16": use_fp16,
        "batch_size": batch_size,
        "yolo_imgsz": yolo_imgsz,
        "yolo_conf": yolo_conf,
    }

    # Add emotion list info to sidebar
    # Add emotion list info to sidebar with Russian translations
    emotion_translations = {
        "Angry": "–ó–ª–æ—Å—Ç—å",
        "Disgust": "–û—Ç–≤—Ä–∞—â–µ–Ω–∏–µ",
        "Fear": "–°—Ç—Ä–∞—Ö",
        "Happy": "–°—á–∞—Å—Ç—å–µ",
        "Sad": "–ì—Ä—É—Å—Ç—å",
        "Surprise": "–£–¥–∏–≤–ª–µ–Ω–∏–µ",
        "Neutral": "–ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ",
    }
    emotion_list = [f"{e} ({emotion_translations.get(e, '')})" for e in EMOTION_OPTIONS]
    st.sidebar.info("**–≠–º–æ—Ü–∏–∏:**\n" + ", ".join(emotion_list))

    page = st.session_state.get(STATE["page"], PAGES["processing"])
    if page == PAGES["processing"]:
        page_processing(perf_opts)
    elif page == PAGES["review"]:
        page_review()
    else:
        st.session_state[STATE["page"]] = PAGES["processing"]
        page_processing(perf_opts)


if __name__ == "__main__":
    main()